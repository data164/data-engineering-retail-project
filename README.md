# Pipeline Data Engineering - Retail Analytics

## Ã€ propos du projet

Projet personnel de pipeline ETL complet pour analyser des donnÃ©es retail. L'idÃ©e Ã©tait de crÃ©er quelque chose de concret, de bout en bout, pour comprendre vraiment comment fonctionne un pipeline de donnÃ©es en production.

**Objectif** : Automatiser l'ingestion et la transformation de donnÃ©es retail pour gÃ©nÃ©rer des KPIs business utilisables (CA, stocks, pricing, promotions).

---

## Architecture

```
ğŸ“‚ DonnÃ©es Brutes (CSV)
    â†“
â˜ï¸ Google Cloud Storage (Data Lake)
    â†“
ğŸ”„ Apache Airflow (Orchestration)
    â†“
ğŸ“Š BigQuery (Data Warehouse)
    â†“ Transformation SQL
ğŸ“ˆ Data Mart (KPIs quotidiens)
    â†“
ğŸ“Š Power BI (Visualisation)
```

### Technologies utilisÃ©es

- **Orchestration** : Apache Airflow 2.7.3 (dÃ©ployÃ© localement avec Docker)
- **Cloud** : Google Cloud Platform (Storage + BigQuery)
- **Transformation** : SQL
- **Visualisation** : Power BI
- **Containerisation** : Docker Compose

---

## ğŸ“ Structure du Projet

```
data-engineering-retail-project/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ dag_retail.py              # DAG Airflow pour l'orchestration ETL
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_kpis.sql            # RequÃªte de transformation des KPIs
â”‚
â”œâ”€â”€ docker-compose.yaml            # Configuration Docker Airflow
â”‚
â”œâ”€â”€ powerbi/
â”‚   â””â”€â”€ retail_dashboard.pbix      # Dashboard Power BI
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ gcp-key-example.json       # Template de clÃ© de service GCP
â”‚
â””â”€â”€ README.md
```

---

## Ce que fait le pipeline

### 1. Ingestion des donnÃ©es
Les donnÃ©es retail (transactions, stocks, produits) sont d'abord uploadÃ©es sur Google Cloud Storage, puis chargÃ©es dans BigQuery. J'ai dÃ» gÃ©rer des petits problÃ¨mes de format CSV (sÃ©parateur `;` au lieu de `,`) car les donnÃ©es venaient d'un dataset anglophone que j'ai adaptÃ©.

### 2. Orchestration avec Airflow
J'ai configurÃ© Airflow en local avec Docker Compose pour automatiser le pipeline. Le DAG tourne tous les jours et rÃ©gÃ©nÃ¨re la table de KPIs Ã  partir des donnÃ©es brutes. Ã‡a m'a pris un moment pour bien configurer la connexion avec GCP (service account, IAM, tout Ã§a).

### 3. Transformation SQL
La partie la plus intÃ©ressante : transformer les donnÃ©es brutes en mÃ©triques business exploitables. J'agrÃ¨ge par date, magasin, produit et contexte (mÃ©tÃ©o, promotions, saisonnalitÃ©) pour calculer :
- Chiffre d'affaires et volumes de vente
- Niveaux de stock et prÃ©visions de demande
- Prix moyens et comparaisons avec la concurrence
- Impact des promotions

### 4. Visualisation Power BI
J'ai crÃ©Ã© 3 dashboards pour analyser les rÃ©sultats :

**Performance Globale**
- CA total : 550Mâ‚¬ sur la pÃ©riode analysÃ©e
- 10M d'unitÃ©s vendues
- Marge brute de 450Mâ‚¬ (taux de marge 81,83%)
- Ã‰volution du CA par catÃ©gorie produit

**Gestion Stocks & Promotions**
- Couverture stock : 194 jours en moyenne
- ROI des promotions : 450%
- Ã‰cart entre prÃ©visions et ventes rÃ©elles : 3,6%

**Analyse CompÃ©titivitÃ©**
- Comparaison de nos prix vs concurrence par catÃ©gorie
- Ã‰cart moyen : -0,02% (on est lÃ©gÃ¨rement moins chers)

---

## Insights Business & Recommandations

Au-delÃ  de la stack technique, l'exploitation de ces donnÃ©es a permis d'identifier plusieurs constats critiques et opportunitÃ©s stratÃ©giques pour le retail.

### Baisse du Chiffre d'Affaires (2022-2023)

Le CA a significativement diminuÃ© entre 2022 et 2023. Quand on voit la courbe descendre comme Ã§a, Ã§a sent pas bon. C'est le type de signal qui doit faire tiquer n'importe quel directeur commercial et nÃ©cessite une analyse approfondie des causes : perte de parts de marchÃ© face Ã  la concurrence, stratÃ©gie pricing inadaptÃ©e, mix produit Ã  revoir, ou impact d'Ã©vÃ©nements externes (inflation, pouvoir d'achat).

Action prioritaire : identifier rapidement les leviers de redressement avant que la tendance ne s'aggrave.

### Surstock Critique (194% de couverture)

La couverture de stock moyenne atteint 194%, soit presque 2 fois plus de stock que nÃ©cessaire.

ConsÃ©quences business :
- Capital immobilisÃ© : ressources financiÃ¨res bloquÃ©es qui pourraient Ãªtre investies ailleurs
- CoÃ»ts de stockage : loyers d'entrepÃ´t, manutention, assurances qui explosent
- Risque de dÃ©marque : obsolescence des produits, dÃ©tÃ©rioration, invendus Ã  brader

Calcul approximatif : avec un CA de 550Mâ‚¬ et un taux de marge de 82%, avoir 94% de stock en trop reprÃ©sente environ 38% du capital immobilisÃ© inutilement.

Recommandation : rÃ©duire les commandes fournisseurs pour ajuster le stock au niveau optimal, accÃ©lÃ©rer la rotation via des promotions ciblÃ©es sur les produits Ã  forte couverture, amÃ©liorer la prÃ©cision des prÃ©visions de demande.

### ROI Promotionnel Exceptionnel (450%)

Chaque euro investi en promotion rapporte 4,50â‚¬. C'est Ã©norme. Ce ROI trÃ¨s Ã©levÃ© n'est pas juste un chiffre, c'est une opportunitÃ© stratÃ©gique majeure sous-exploitÃ©e.

Recommandation stratÃ©gique : avec un ROI pareil, on pourrait baisser lÃ©gÃ¨rement les prix de base (exemple : -3 Ã  -5%) et compenser avec encore plus de promotions (frÃ©quence et agressivitÃ© accrues).

RÃ©sultat attendu : prix affichÃ©s plus attractifs que la concurrence, gÃ©nÃ©ration de volume de ventes (et c'est rentable vu le ROI), accÃ©lÃ©ration de la rotation des stocks (bye bye le surstock), marge maintenue voire amÃ©liorÃ©e grÃ¢ce au ROI Ã©levÃ©.

Benchmark : c'est exactement la stratÃ©gie utilisÃ©e par Decathlon, H&M ou Kiabi. Prix corrects + promotions frÃ©quentes = trafic + volume + fidÃ©lisation.

### PrÃ©visions de Demande Performantes (3,6% d'Ã©cart)

Le modÃ¨le de prÃ©vision affiche un Ã©cart moyen de seulement 3,6% entre les prÃ©visions et les ventes rÃ©elles. C'est plutÃ´t bon.

Cependant, une optimisation supplÃ©mentaire permettrait de rÃ©duire encore l'Ã©cart (objectif < 2%), amÃ©liorer la prÃ©cision des commandes fournisseurs, et minimiser les ruptures de stock ET le surstock.

Piste d'amÃ©lioration : intÃ©grer des modÃ¨les de Machine Learning (ARIMA, Prophet, ou modÃ¨les supervisÃ©s) pour capturer la saisonnalitÃ© et les tendances plus finement.

### Positionnement Prix Neutre (-0,02% vs concurrent)

Prix moyen : 55,14â‚¬  
Prix concurrent : 55,15â‚¬  
Ã‰cart : -0,02%

On est pile au mÃªme niveau que la concurrence. Ni plus chers, ni moins chers.

Question stratÃ©gique : comment attirer les clients vers nous plutÃ´t que vers eux ?

Recommandation : vu qu'on a un ROI promo de 450%, on peut jouer plus agressivement sur ce levier.

Option 1 : Prix lÃ©gÃ¨rement en dessous + promos frÃ©quentes â†’ image de "bonnes affaires", volume qui augmente, marge compensÃ©e par le ROI promo.

Option 2 : MÃªme prix MAIS promos plus agressives â†’ -30%, -40%, -50% rÃ©guliers, crÃ©e du trafic et de l'urgence, fidÃ©lisation (les clients attendent les promos).

Dans les deux cas, on utilise notre atout (ROI promo Ã©norme) pour se diffÃ©rencier sans dÃ©truire la marge.

Parce que la vraie question c'est pas "quel prix ?" mais "pourquoi acheter chez nous ?". Et avec les bonnes donnÃ©es, on peut rÃ©pondre Ã  cette question avec des chiffres, pas des intuitions.

### SynthÃ¨se des Leviers d'Action

| ProblÃ¨me IdentifiÃ© | Impact Business | Action RecommandÃ©e |
|---------------------|-----------------|-------------------|
| Baisse CA 2022-2023 | Perte de revenus, alerte stratÃ©gique | Analyse causes racines + plan d'action commercial urgent |
| Surstock 194% | Capital immobilisÃ© (38%), coÃ»ts de stockage Ã©levÃ©s | RÃ©duction commandes + promotions ciblÃ©es |
| ROI promo 450% | OpportunitÃ© sous-exploitÃ©e | StratÃ©gie pricing dynamique (prix base en baisse, promos en hausse) |
| Prix = concurrent | Manque de diffÃ©renciation | Exploiter le levier promotionnel pour se dÃ©marquer |
| Ã‰cart prÃ©visions 3,6% | Optimisation possible | AmÃ©liorer les modÃ¨les de forecast (ML) |

### Ce que Ã§a m'a appris

La data dans le retail, c'est pas juste des chiffres et des dashboards. C'est comprendre des enjeux business concrets : marge, rotation des stocks, Ã©lasticitÃ© prix, saisonnalitÃ©. C'est transformer des lignes Excel en dÃ©cisions stratÃ©giques actionnables qui impactent directement le CA, les stocks, et la rentabilitÃ©.

Et c'est exactement ce qui me plaÃ®t dans la data.

---

## Installation

Si vous voulez reproduire le projet ou l'adapter, voici les Ã©tapes principales.

### PrÃ©requis
- Docker et Docker Compose
- Un compte Google Cloud Platform (gratuit pour commencer)
- Python 3.8+
- Power BI Desktop pour la visualisation

### Configuration rapide

**1. Setup GCP**

CrÃ©er un projet et activer les APIs BigQuery et Cloud Storage :
```bash
gcloud projects create pipeline-etl-retail
gcloud services enable bigquery.googleapis.com storage.googleapis.com
```

CrÃ©er un service account avec les permissions nÃ©cessaires et tÃ©lÃ©charger la clÃ© JSON.

**2. Lancer Airflow**

Cloner le repo et dÃ©marrer les conteneurs Docker :
```bash
git clone https://github.com/data164/data-engineering-retail-project.git
cd data-engineering-retail-project

# Placer la clÃ© GCP dans config/gcp-key.json
docker-compose up -d
```

Interface Airflow accessible sur `http://localhost:8080` (login: airflow / airflow)

**3. Charger les donnÃ©es**

Uploader le fichier CSV dans Cloud Storage, puis crÃ©er la table dans BigQuery :
```sql
LOAD DATA INTO \`pipeline-etl-retail.retail_data_warehouse.raw_retail\`
FROM FILES (
  format = 'CSV',
  uris = ['gs://bucket-retailll/retail_store_data.csv'],
  skip_leading_rows = 1,
  field_delimiter = ';'
);
```

**4. ExÃ©cuter le pipeline**

Le DAG s'exÃ©cute automatiquement chaque jour, ou vous pouvez le trigger manuellement depuis l'interface Airflow.

---

## RequÃªte SQL principale

La transformation agrÃ¨ge les donnÃ©es brutes pour crÃ©er une table de KPIs quotidiens. Voici la logique :

```sql
CREATE OR REPLACE TABLE \`pipeline-etl-retail.retail_data_warehouse.retail_daily_kpis\` AS
SELECT
    -- Dimensions pour le groupement
    Date,
    \`Store ID\` AS store_id,
    \`Product ID\` AS product_id,
    Category AS product_category,
    Region AS store_region,
    \`Weather Condition\` AS weather_condition,
    \`Holiday_Promotion\` AS holiday_promotion_status,
    Seasonality AS seasonality,

    -- MÃ©triques de vente
    ROUND(SUM(\`Units Sold\`), 2) AS total_units_sold,
    ROUND(SUM(Price * \`Units Sold\`), 2) AS daily_revenue,

    -- MÃ©triques de stock
    ROUND(AVG(\`Inventory Level\`), 2) AS average_inventory_level,
    ROUND(SUM(\`Units Ordered\`), 2) AS total_units_ordered,
    ROUND(AVG(\`Demand Forecast\`), 2) AS average_demand_forecast,

    -- MÃ©triques de prix
    ROUND(AVG(Price), 2) AS average_product_price,
    ROUND(AVG(Discount), 2) AS average_discount_rate,
    ROUND(AVG(\`Competitor Pricing\`), 2) AS average_competitor_price

FROM \`pipeline-etl-retail.retail_data_warehouse.raw_retail\`
GROUP BY Date, \`Store ID\`, \`Product ID\`, Category, Region, 
         \`Weather Condition\`, \`Holiday_Promotion\`, Seasonality
ORDER BY Date, \`Store ID\`, \`Product ID\`;
```

Rien de complexe, mais Ã§a permet d'avoir une table propre et agrÃ©gÃ©e pour alimenter Power BI.

---

## Mesures DAX (Power BI)

Quelques exemples de mesures calculÃ©es dans Power BI :

**Chiffre d'Affaires Total**
```dax
CA Total = 
ROUND(
    SUMX(retail_kpi_export, VALUE(SUBSTITUTE([daily_revenue], ".", ","))),
    0
)
```

**Marge Brute**
```dax
Marge Brute = [CA Total] - [CoÃ»t Promotions]
```

**ROI Promotions**
```dax
ROI Promotions = 
ROUND(
    VAR CATotal = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([daily_revenue], ".", ",")))
    VAR CoutPromo = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([total_discount_given], ".", ",")))
    RETURN DIVIDE(CATotal - CoutPromo, CoutPromo, 0) * 100,
    1
)
```

**Ã‰cart PrÃ©visionnel**
```dax
Ã‰cart PrÃ©visionnel % = 
VAR VentesReelles = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([total_units_sold], ".", ",")))
VAR Previsions = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([average_demand_forecast], ".", ",")))
VAR Ecart = ABS(VentesReelles - Previsions)
RETURN ROUND(DIVIDE(Ecart, Previsions, 0) * 100, 1)
```

J'ai dÃ» utiliser SUBSTITUTE pour convertir les points en virgules (problÃ¨me de format entre BigQuery et Power BI).

---

## Applications concrÃ¨tes

Le pipeline permet de rÃ©pondre Ã  plusieurs questions business :

**Optimisation des promotions**  
Les promotions sont-elles rentables ? â†’ ROI de 450% calculÃ© automatiquement

**Gestion des stocks**  
Risque de rupture ou de surstock ? â†’ Indicateur de couverture (194 jours) vs demande prÃ©vue

**Positionnement prix**  
Sommes-nous compÃ©titifs ? â†’ Comparaison automatique avec les prix concurrents par catÃ©gorie

**Analyse de performance**  
Quels produits/rÃ©gions performent le mieux ? â†’ Dashboards avec drill-down par catÃ©gorie et temporalitÃ©

---

## AmÃ©liorations possibles

Quelques idÃ©es pour aller plus loin (que je n'ai pas eu le temps d'implÃ©menter) :

- Ajouter des tests automatisÃ©s pour vÃ©rifier la qualitÃ© des donnÃ©es
- Migrer vers Cloud Composer (Airflow managÃ© sur GCP)
- ImplÃ©menter dbt pour mieux gÃ©rer les transformations SQL
- Ajouter des alertes Slack en cas d'anomalie ou d'Ã©chec du pipeline
- CrÃ©er des modÃ¨les de prÃ©vision ML pour amÃ©liorer la demande forecast

---

## Ce que j'ai appris

Ce projet m'a permis de comprendre concrÃ¨tement :
- Comment orchestrer un pipeline avec Airflow (galÃ¨res avec Docker et les connexions GCP incluses)
- L'architecture cloud moderne (Storage, Warehouse, IAM)
- L'importance de la qualitÃ© des donnÃ©es (les problÃ¨mes de format CSV m'ont fait perdre du temps)
- Comment connecter les diffÃ©rents outils entre eux (GCP, Airflow, Power BI)

Le plus dur n'Ã©tait pas le code mais la config et l'intÃ©gration des diffÃ©rents services.

---

## Contact

**Mohamed Amhamed**  
Ã‰tudiant IngÃ©nieur Data - CESI Lille (Bac+3)

Email : mohamed.amhamed@viacesi.fr  
LinkedIn : [linkedin.com/in/mohamed-amhamed](https://linkedin.com/in/mohamed-amhamed)  
GitHub : [github.com/data164](https://github.com/data164)

N'hÃ©sitez pas Ã  me contacter pour discuter du projet ou Ã©changer sur le data engineering !

---

*Projet rÃ©alisÃ© en Octobre 2025 dans le cadre de mon apprentissage du data engineering*
