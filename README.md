# Pipeline Data Engineering - Retail Analytics

## √Ä propos du projet

Projet personnel de pipeline ETL complet pour analyser des donn√©es retail. L'id√©e √©tait de cr√©er quelque chose de concret, de bout en bout, pour comprendre vraiment comment fonctionne un pipeline de donn√©es en production.

**Objectif** : Automatiser l'ingestion et la transformation de donn√©es retail pour g√©n√©rer des KPIs business utilisables (CA, stocks, pricing, promotions).

---

## Architecture

```
üìÇ Donn√©es Brutes (CSV)
    ‚Üì
‚òÅÔ∏è Google Cloud Storage (Data Lake)
    ‚Üì
üîÑ Apache Airflow (Orchestration)
    ‚Üì
üìä BigQuery (Data Warehouse)
    ‚Üì Transformation SQL
üìà Data Mart (KPIs quotidiens)
    ‚Üì
üìä Power BI (Visualisation)
```

### Technologies utilis√©es

- **Orchestration** : Apache Airflow 2.7.3 (d√©ploy√© localement avec Docker)
- **Cloud** : Google Cloud Platform (Storage + BigQuery)
- **Transformation** : SQL
- **Visualisation** : Power BI
- **Containerisation** : Docker Compose

---

## üìÅ Structure du Projet

```
data-engineering-retail-project/
‚îÇ
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îî‚îÄ‚îÄ dag_retail.py              # DAG Airflow pour l'orchestration ETL
‚îÇ
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îî‚îÄ‚îÄ create_kpis.sql            # Requ√™te de transformation des KPIs
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yaml            # Configuration Docker Airflow
‚îÇ
‚îú‚îÄ‚îÄ powerbi/
‚îÇ   ‚îî‚îÄ‚îÄ retail_dashboard.pbix      # Dashboard Power BI
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ gcp-key-example.json       # Template de cl√© de service GCP
‚îÇ
‚îî‚îÄ‚îÄ README.md
```

---

## Ce que fait le pipeline

### 1. Ingestion des donn√©es
Les donn√©es retail (transactions, stocks, produits) sont d'abord upload√©es sur Google Cloud Storage, puis charg√©es dans BigQuery. J'ai d√ª g√©rer des petits probl√®mes de format CSV (s√©parateur `;` au lieu de `,`) car les donn√©es venaient d'un dataset anglophone que j'ai adapt√©.

### 2. Orchestration avec Airflow
J'ai configur√© Airflow en local avec Docker Compose pour automatiser le pipeline. Le DAG tourne tous les jours et r√©g√©n√®re la table de KPIs √† partir des donn√©es brutes. √áa m'a pris un moment pour bien configurer la connexion avec GCP (service account, IAM, tout √ßa).

### 3. Transformation SQL
La partie la plus int√©ressante : transformer les donn√©es brutes en m√©triques business exploitables. J'agr√®ge par date, magasin, produit et contexte (m√©t√©o, promotions, saisonnalit√©) pour calculer :
- Chiffre d'affaires et volumes de vente
- Niveaux de stock et pr√©visions de demande
- Prix moyens et comparaisons avec la concurrence
- Impact des promotions

### 4. Visualisation Power BI
J'ai cr√©√© 3 dashboards pour analyser les r√©sultats :

**Performance Globale**
- CA total : 550M‚Ç¨ sur la p√©riode analys√©e
- 10M d'unit√©s vendues
- Marge brute de 450M‚Ç¨ (taux de marge 81,83%)
- √âvolution du CA par cat√©gorie produit

**Gestion Stocks & Promotions**
- Couverture stock : 194 jours en moyenne
- ROI des promotions : 450%
- √âcart entre pr√©visions et ventes r√©elles : 3,6%

**Analyse Comp√©titivit√©**
- Comparaison de nos prix vs concurrence par cat√©gorie
- √âcart moyen : -0,02% (on est l√©g√®rement moins chers)

---

## Installation

Si vous voulez reproduire le projet ou l'adapter, voici les √©tapes principales.

### Pr√©requis
- Docker et Docker Compose
- Un compte Google Cloud Platform (gratuit pour commencer)
- Python 3.8+
- Power BI Desktop pour la visualisation

### Configuration rapide

**1. Setup GCP**

Cr√©er un projet et activer les APIs BigQuery et Cloud Storage :
```bash
gcloud projects create pipeline-etl-retail
gcloud services enable bigquery.googleapis.com storage.googleapis.com
```

Cr√©er un service account avec les permissions n√©cessaires et t√©l√©charger la cl√© JSON.

**2. Lancer Airflow**

Cloner le repo et d√©marrer les conteneurs Docker :
```bash
git clone https://github.com/data164/data-engineering-retail-project.git
cd data-engineering-retail-project

# Placer la cl√© GCP dans config/gcp-key.json
docker-compose up -d
```

Interface Airflow accessible sur `http://localhost:8080` (login: airflow / airflow)

**3. Charger les donn√©es**

Uploader le fichier CSV dans Cloud Storage, puis cr√©er la table dans BigQuery :
```sql
LOAD DATA INTO `pipeline-etl-retail.retail_data_warehouse.raw_retail`
FROM FILES (
  format = 'CSV',
  uris = ['gs://bucket-retailll/retail_store_data.csv'],
  skip_leading_rows = 1,
  field_delimiter = ';'
);
```

**4. Ex√©cuter le pipeline**

Le DAG s'ex√©cute automatiquement chaque jour, ou vous pouvez le trigger manuellement depuis l'interface Airflow.

---

## Requ√™te SQL principale

La transformation agr√®ge les donn√©es brutes pour cr√©er une table de KPIs quotidiens. Voici la logique :

```sql
CREATE OR REPLACE TABLE `pipeline-etl-retail.retail_data_warehouse.retail_daily_kpis` AS
SELECT
    -- Dimensions pour le groupement
    Date,
    `Store ID` AS store_id,
    `Product ID` AS product_id,
    Category AS product_category,
    Region AS store_region,
    `Weather Condition` AS weather_condition,
    `Holiday_Promotion` AS holiday_promotion_status,
    Seasonality AS seasonality,

    -- M√©triques de vente
    ROUND(SUM(`Units Sold`), 2) AS total_units_sold,
    ROUND(SUM(Price * `Units Sold`), 2) AS daily_revenue,

    -- M√©triques de stock
    ROUND(AVG(`Inventory Level`), 2) AS average_inventory_level,
    ROUND(SUM(`Units Ordered`), 2) AS total_units_ordered,
    ROUND(AVG(`Demand Forecast`), 2) AS average_demand_forecast,

    -- M√©triques de prix
    ROUND(AVG(Price), 2) AS average_product_price,
    ROUND(AVG(Discount), 2) AS average_discount_rate,
    ROUND(AVG(`Competitor Pricing`), 2) AS average_competitor_price

FROM `pipeline-etl-retail.retail_data_warehouse.raw_retail`
GROUP BY Date, `Store ID`, `Product ID`, Category, Region, 
         `Weather Condition`, `Holiday_Promotion`, Seasonality
ORDER BY Date, `Store ID`, `Product ID`;
```

Rien de complexe, mais √ßa permet d'avoir une table propre et agr√©g√©e pour alimenter Power BI.

---

## Mesures DAX (Power BI)

Quelques exemples de mesures calcul√©es dans Power BI :

**Chiffre d'Affaires Total**
```dax
CA Total = 
ROUND(
    SUMX(retail_kpi_export, VALUE(SUBSTITUTE([daily_revenue], ".", ","))),
    0
)
```

**Marge Brute**
```dax
Marge Brute = [CA Total] - [Co√ªt Promotions]
```

**ROI Promotions**
```dax
ROI Promotions = 
ROUND(
    VAR CATotal = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([daily_revenue], ".", ",")))
    VAR CoutPromo = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([total_discount_given], ".", ",")))
    RETURN DIVIDE(CATotal - CoutPromo, CoutPromo, 0) * 100,
    1
)
```

**√âcart Pr√©visionnel**
```dax
√âcart Pr√©visionnel % = 
VAR VentesReelles = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([total_units_sold], ".", ",")))
VAR Previsions = SUMX(retail_kpi_export, VALUE(SUBSTITUTE([average_demand_forecast], ".", ",")))
VAR Ecart = ABS(VentesReelles - Previsions)
RETURN ROUND(DIVIDE(Ecart, Previsions, 0) * 100, 1)
```

J'ai d√ª utiliser SUBSTITUTE pour convertir les points en virgules (probl√®me de format entre BigQuery et Power BI).

---

## Applications concr√®tes

Le pipeline permet de r√©pondre √† plusieurs questions business :

**Optimisation des promotions**  
Les promotions sont-elles rentables ? ‚Üí ROI de 450% calcul√© automatiquement

**Gestion des stocks**  
Risque de rupture ou de surstock ? ‚Üí Indicateur de couverture (194 jours) vs demande pr√©vue

**Positionnement prix**  
Sommes-nous comp√©titifs ? ‚Üí Comparaison automatique avec les prix concurrents par cat√©gorie

**Analyse de performance**  
Quels produits/r√©gions performent le mieux ? ‚Üí Dashboards avec drill-down par cat√©gorie et temporalit√©

---

## Am√©liorations possibles

Quelques id√©es pour aller plus loin (que je n'ai pas eu le temps d'impl√©menter) :

- Ajouter des tests automatis√©s pour v√©rifier la qualit√© des donn√©es
- Migrer vers Cloud Composer (Airflow manag√© sur GCP)
- Impl√©menter dbt pour mieux g√©rer les transformations SQL
- Ajouter des alertes Slack en cas d'anomalie ou d'√©chec du pipeline
- Cr√©er des mod√®les de pr√©vision ML pour am√©liorer la demande forecast

---

## Ce que j'ai appris

Ce projet m'a permis de comprendre concr√®tement :
- Comment orchestrer un pipeline avec Airflow (gal√®res avec Docker et les connexions GCP incluses)
- L'architecture cloud moderne (Storage, Warehouse, IAM)
- L'importance de la qualit√© des donn√©es (les probl√®mes de format CSV m'ont fait perdre du temps)
- Comment connecter les diff√©rents outils entre eux (GCP, Airflow, Power BI)

Le plus dur n'√©tait pas le code mais la config et l'int√©gration des diff√©rents services.

---

## Contact

**Mohamed Amhamed**  
√âtudiant Ing√©nieur Data - CESI Lille (Bac+3)

Email : mohamed.amhamed@viacesi.fr  
LinkedIn : [linkedin.com/in/mohamed-amhamed](https://linkedin.com/in/mohamed-amhamed)  
GitHub : [github.com/data164](https://github.com/data164)

N'h√©sitez pas √† me contacter pour discuter du projet ou √©changer sur le data engineering !

---

*Projet r√©alis√© en Octobre 2025 dans le cadre de mon apprentissage du data engineering*
